# Breeze ASR 25

<img src="https://huggingface.co/MediaTek-Research/Twister/resolve/main/BreezeASR25.png" alt="Breeze ASR 25" width="700"/>


**Breeze ASR 25** æ˜¯ä¸€æ¬¾åŸºæ–¼ Whisper-large-v2 é–‹ç™¼çš„èªéŸ³è¾¨è­˜æ¨¡å‹ï¼Œä¸¦å…·æœ‰ä»¥ä¸‹ç‰¹è‰²ï¼š

- å¼·åŒ–ç¹é«”ä¸­æ–‡èªå¢ƒè¾¨è­˜èƒ½åŠ›
- æ¡ç”¨å–®ä¸€æ··å’Œèªè¨€å‘é‡è§£ç¢¼ï¼Œå¼·åŒ–ä¸­è‹±äº¤éŒ¯æƒ…å¢ƒèªå¢ƒè¾¨è­˜èƒ½åŠ›ï¼ŒåŒ…å«å¥å…§ä»¥åŠå¥å¤–è½‰æ›
- å¼·åŒ–æ™‚é–“æˆ³è¨˜å°é½Šï¼Œé©åˆè‡ªå‹•å­—å¹•ç”Ÿæˆ

**Breeze ASR 25** is an advanced ASR model fine-tuned from [Whisper-large-v2](https://github.com/openai/whisper) 


- Optimized for Taiwanese Mandarin
- Adopted an unified mix embedding for decoding, optimized for Mandarin-English code-switching scenarios, including intra-sentential switching and inter-sentential switching.
- Enhanced time alignment, suitable for automatic captioning

---
## Example:

å¢å¼·ç¯„ä¾‹-ä¸­è‹±æ··ç”¨æƒ…å¢ƒï¼š [MediaTek's 24th Anniversary](https://www.youtube.com/watch?v=YkUv5qyhVhw&t=261s)

Breeze ASR 25:

```
é¢å°ä¸çŸ¥é“çš„æˆ‘å€‘æ€éº¼ç”¨ open mind open heart çš„å¿ƒæƒ…å» explore
é‚£ explore éç¨‹ä¹Ÿå°±æ˜¯æŒçºŒå­¸ç¿’ ä¸æ–·å‰µæ–°
ç•¶ç„¶å¦‚æœèƒ½å¸¶é ˜ MediaTek èªªé”åˆ°é€™æ¨£çš„ position
å°åšé€™æ¨£çš„äº‹æƒ…é‚£è¦ºå¾—æ˜¯ä¸€å€‹ commitment
é‚£ä¹Ÿæ˜¯ä¸€å€‹ passion é‚£å¯ä»¥ä¸€ç›´å¾ˆåŠªåŠ›çš„æŠ•å…¥åœ¨åš
```

Whisper-large-v2:

```
é¢å°ä¸çŸ¥é“çš„æˆ‘å€‘æ€éº¼ç”¨é–‹æ”¾å¿ƒæƒ…å»æ¢ç´¢
æŠŠå®ƒæ¢ç´¢éç¨‹ä¹Ÿå°±æ˜¯ ä»”ç´°å­¸ç¿’ ä¸æ–·å‰µæ–°
ç•¶ç„¶å¦‚æœèƒ½å¸¶é ˜MediaTekèªª é”åˆ°é€™æ¨£çš„å±¤æ¬¡ å°åšé€™æ¨£çš„äº‹æƒ…
é‚£è¦ºå¾—æ˜¯ä¸€å€‹è²¢ç»é‚£ä¹Ÿæ˜¯ä¸€å€‹ç†±èª 
é‚£å¯ä»¥ä¸€ç›´ä¾†åŠªåŠ›åœ°æŠ•å…¥åœ¨åš
```

---

## Performance
The WERR is reported in comparison with the Whisper-large-v2 automatic language detection (WLV2-Auto) baseline. "Breeze ASR 25" is refered in the [paper](https://arxiv.org/pdf/2506.11130) as "Twister"
### Short-form Audio Datasets

| Dataset\Model             | Language | WLV2-Auto â†“ | WLV3-Auto â†“ | COOL-Whisper â†“ | **Breeze ASR 25 (Ours)** â†“ |
|---------------------------|---------------|-------------|-------------|----------------|------------------|
| ASCEND-OVERALL*           | Mixed | 21.14       | 23.22       | 19.71          | **17.74** (-16.08%) |
| - ASCEND-EN               | English    | 27.36       | 27.21       | 29.39          | **26.64** (-2.63%)  |
| - ASCEND-ZH               | Mandarin | 17.49       | 17.41       | 18.90          | **16.04** (-8.29%)     |
| - ASCEND-MIX*             | Mixed  | 21.01       | 25.13       | 17.34          | **16.38** (-22.01%) |
| CommonVoice16-zh-TW       | Mandarin     | 9.84        | 8.95        | 11.86          | **7.97** (-19%)     |
| CSZS-zh-en*               | Mixed  | 29.49       | 26.43       | 20.90          | **13.01** (-55.88%) |

### Long-form Audio Datasets

| Dataset\Model             | Language | WLV2-Auto â†“ | WLV3-Auto â†“ | COOL-Whisper â†“ | **Breeze ASR 25 (Ours)** â†“ |
|---------------------------|---------------|-------------|-------------|----------------|------------------|
| ML-lecture-2021-long*     | Mandarin     | 6.13        | 6.41        | 6.37           | **4.98** (-18.76%) |
| Formosa-Go                | Mandarin    | 15.03       | 14.90       | 16.83          | **13.61** (-9.44%) |
| Formosa-Show              | Mandarin   | 29.18       | 27.80       | 29.78          | **27.58** (-5.48%) |
| Formosa-Course            | Mandarin | **9.50**       | 9.67        | 11.12          | 9.94 (+0.44%)      |
| Formosa-General           | Mandarin    | 11.45       | 11.46       | 13.33          | **11.37** (-0.69%) |
| FormosaSpeech             | Mandarin   | 22.34       | 21.22       | 26.71          | **22.09** (-1.12%) |

\* Code-switching datasets

---

## Training Data

æ‰€æœ‰ Twister çš„çš„è¨“ç·´å–æ¨£è‡ª**å¯¬é¬†è‡ªç”±è»Ÿé«”æˆæ¬Šæ¢æ¬¾**çš„æ•¸æ“šé›†ï¼Œä¸­æ–‡éƒ¨åˆ†å®Œå…¨æ¡ç”¨åˆæˆèªéŸ³è³‡æ–™ï¼š

The training data of Twister is sampled from the following publicly available sources with **permissive open-source licenses**, where all Chinese data are synthetic:


| Dataset Name                                                                 | Type   | Language        | Total Hours | License |
|------------------------------------------------------------------------------|--------|-----------------|-------------|---------|
| ODC Synth                                                                    | Synthetic | Mandarin        | 10,000      | Open Data Commons License Attribution + Apache2.0* |
| [CommonVoice17-EN](https://huggingface.co/datasets/mozilla-foundation/common_voice_17_0) | Real   | English         | 1,738       | Creative Commons Zero |
| [NTUML2021](https://huggingface.co/datasets/ky552/ML2021_ASR_ST)              | Real   | Code-switching  | 11          | MIT License |


*ODC Synth is generated by using text from [FineWeb2](https://huggingface.co/datasets/HuggingFaceFW/fineweb-2) (ODC License) and a TTS model [BreezyVoice](https://huggingface.co/MediaTek-Research/BreezyVoice) (Apache2.0 License)

Additional code-switching samples are generated through data augmentation with these three datasets; further details can be found in our [paper](https://arxiv.org/pdf/2506.11130).

---

## ğŸ”§ Usage Example

The whisper architecture is supported in Hugging Face ğŸ¤— Transformers.

First, install relavant packages:

```
pip install --upgrade pip
pip install --upgrade transformers datasets[audio] accelerate
```

It is advised to run the model with `pipeline`, which supports arbitrary length.
Sequential modeling (`chunk_length_s=0`) yields best results.

```python
python run.py --file_name=FILE_NAME
```

---

## Acknowledgements

We thank NVIDIA for providing access to the Taipei-1 supercomputer. 

We thank Professor Hung-yi Lee for his valuable guidance on this project.

---

## ğŸ“œ Citation

If you find this model useful, please cite our work:

**Cheng-Kang Chou\***, **Chan-Jan Hsu\***, Ho-Lam Chung, Liang-Hsuan Tseng, Hsi-Chun Cheng, Yu-Kuan Fu, Kuan-Po Huang, Hung-yi Lee  
[*A Self-Refining Framework for Enhancing ASR Using TTS-Synthesized Data*](https://arxiv.org/pdf/2506.11130)

\*Equal contribution 

```bibtex
@article{chou2025selfrefiningframeworkenhancingasr,
  title={A Self-Refining Framework for Enhancing ASR Using TTS-Synthesized Data},
  author={Cheng Kang Chou and Chan-Jan Hsu and Ho-Lam Chung and Liang-Hsuan Tseng and Hsi-Chun Cheng and Yu-Kuan Fu and Kuan Po Huang and Hung-Yi Lee},
  journal={arXiv preprint arXiv:2506.11130},
  year={2025}
}
```
